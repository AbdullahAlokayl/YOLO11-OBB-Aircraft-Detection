{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO11 OBB Model Training and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This section of the notebook initializes the necessary libraries and sets up the environment for training a YOLO model using oriented bounding box (OBB) annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import shutil\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert XML to YOLO Oriented Bounding Box (OBB) Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This Python script converts XML annotation files (Pascal VOC format) with oriented bounding boxes (OBB) into the YOLO OBB format. It reads annotation files from a specified directory, extracts object labels and bounding box coordinates, normalizes them relative to image dimensions, and saves them in YOLO OBB format. The script includes error handling for missing or invalid data and provides warnings for unrecognized classes. This tool is useful for preparing datasets for YOLO-based object detection models that support oriented bounding boxes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set paths\n",
    "xml_dir = \"dataset/Annotations/Oriented Bounding Boxes\"  # Update this to your XML folder\n",
    "output_dir = \"dataset/Annotations/yolo-obb-format\"  # Update this to your YOLO labels folder\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define class mapping (update based on your dataset)\n",
    "class_mapping = {\n",
    "    \"A1\": 0, \"A10\": 1, \"A11\": 2, \"A12\": 3, \"A13\": 4, \"A14\": 5, \"A15\": 6, \"A16\": 7, \"A17\": 8, \"A18\": 9,\n",
    "    \"A19\": 10, \"A2\": 11, \"A20\": 12, \"A3\": 13, \"A4\": 14, \"A5\": 15, \"A6\": 16, \"A7\": 17, \"A8\": 18, \"A9\": 19\n",
    "}\n",
    "\n",
    "def convert_xml_to_yolo_obb(xml_file, output_folder):\n",
    "    \"\"\" Convert XML annotation to YOLO OBB format. \"\"\"\n",
    "    tree = ET.parse(xml_file)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Get image filename (without extension)\n",
    "    image_filename = root.find(\"filename\").text\n",
    "    image_name = os.path.splitext(image_filename)[0]\n",
    "\n",
    "    # Get image dimensions (handling missing values)\n",
    "    size = root.find(\"size\")\n",
    "    if size is None:\n",
    "        print(f\"‚ö†Ô∏è Warning: Missing <size> tag in {xml_file}. Skipping file.\")\n",
    "        return\n",
    "    \n",
    "    width = size.find(\"width\")\n",
    "    height = size.find(\"height\")\n",
    "\n",
    "    # Ensure width and height exist and are valid numbers\n",
    "    try:\n",
    "        image_width = int(width.text) if width is not None else None\n",
    "        image_height = int(height.text) if height is not None else None\n",
    "\n",
    "        if not image_width or not image_height:\n",
    "            print(f\"‚ö†Ô∏è Warning: Missing or invalid image dimensions in {xml_file}. Skipping file.\")\n",
    "            return\n",
    "\n",
    "    except ValueError:\n",
    "        print(f\"‚ö†Ô∏è Warning: Invalid width/height in {xml_file}. Skipping file.\")\n",
    "        return\n",
    "\n",
    "    # Prepare output file\n",
    "    output_path = os.path.join(output_folder, f\"{image_name}.txt\")\n",
    "    yolo_annotations = []\n",
    "\n",
    "    # Iterate over each object in the XML file\n",
    "    for obj in root.findall(\"object\"):\n",
    "        class_name = obj.find(\"name\").text\n",
    "\n",
    "        # Check if class exists in mapping\n",
    "        if class_name not in class_mapping:\n",
    "            print(f\"‚ö†Ô∏è Warning: Class '{class_name}' not found in class mapping. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        class_id = class_mapping[class_name]\n",
    "\n",
    "        # Get OBB coordinates\n",
    "        robndbox = obj.find(\"robndbox\")\n",
    "        try:\n",
    "            x1 = float(robndbox.find(\"x_left_top\").text) / image_width\n",
    "            y1 = float(robndbox.find(\"y_left_top\").text) / image_height\n",
    "            x2 = float(robndbox.find(\"x_right_top\").text) / image_width\n",
    "            y2 = float(robndbox.find(\"y_right_top\").text) / image_height\n",
    "            x3 = float(robndbox.find(\"x_right_bottom\").text) / image_width\n",
    "            y3 = float(robndbox.find(\"y_right_bottom\").text) / image_height\n",
    "            x4 = float(robndbox.find(\"x_left_bottom\").text) / image_width\n",
    "            y4 = float(robndbox.find(\"y_left_bottom\").text) / image_height\n",
    "\n",
    "            # Format for YOLO OBB\n",
    "            yolo_annotations.append(f\"{class_id} {x1:.6f} {y1:.6f} {x2:.6f} {y2:.6f} {x3:.6f} {y3:.6f} {x4:.6f} {y4:.6f}\")\n",
    "\n",
    "        except (AttributeError, ValueError):\n",
    "            print(f\"‚ö†Ô∏è Warning: Missing or invalid bounding box values in {xml_file}. Skipping object.\")\n",
    "\n",
    "    # Save annotations to YOLO OBB text file\n",
    "    if yolo_annotations:\n",
    "        with open(output_path, \"w\") as f:\n",
    "            f.write(\"\\n\".join(yolo_annotations))\n",
    "        print(f\"‚úÖ Converted: {xml_file} ‚Üí {output_path}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è No valid annotations found in {xml_file}. Skipping file.\")\n",
    "\n",
    "# Process all XML files\n",
    "xml_files = [f for f in os.listdir(xml_dir) if f.endswith(\".xml\")]\n",
    "for xml_file in xml_files:\n",
    "    convert_xml_to_yolo_obb(os.path.join(xml_dir, xml_file), output_dir)\n",
    "\n",
    "print(\"üöÄ XML to YOLO OBB conversion complete!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YOLO-OBB Dataset Preparation: Train-Validation Split Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This script organizes an object detection dataset by splitting images and their corresponding YOLO-OBB format annotations into training and validation sets. It randomly shuffles the dataset and separates it based on a predefined ratio (80% training, 20% validation). The processed files are then moved to structured directories under Processed_dataset, ensuring easy accessibility for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset paths\n",
    "images_dir = r\"dataset/Images\"  # Folder containing images\n",
    "labels_dir = r\"dataset/Annotations/yolo-obb-format\"  # Folder containing text label files\n",
    "output_dir = r\"dataset/Processed_dataset\"  # Output directory\n",
    "\n",
    "# Train-validation split ratio\n",
    "train_ratio = 0.8  # 80% train\n",
    "val_ratio = 0.2    # 20% validation\n",
    "\n",
    "# Ensure ratios sum to 1\n",
    "assert train_ratio + val_ratio == 1.0, \"Train and validation ratios must sum to 1.0\"\n",
    "\n",
    "# Create output directories\n",
    "for split in [\"train\", \"val\"]:\n",
    "    os.makedirs(os.path.join(output_dir, \"images\", split), exist_ok=True)\n",
    "    os.makedirs(os.path.join(output_dir, \"labels\", split), exist_ok=True)\n",
    "\n",
    "# Collect all image files (supporting .jpg, .png, .jpeg)\n",
    "all_images = [f for f in os.listdir(images_dir) if f.endswith((\".jpg\", \".png\", \".jpeg\"))]\n",
    "random.shuffle(all_images)  # Shuffle dataset for randomness\n",
    "\n",
    "# Calculate split indices\n",
    "total_images = len(all_images)\n",
    "train_end = int(total_images * train_ratio)\n",
    "\n",
    "train_images = all_images[:train_end]\n",
    "val_images = all_images[train_end:]\n",
    "\n",
    "def move_files(image_list, split):\n",
    "    \"\"\" Moves images and corresponding labels to destination folders \"\"\"\n",
    "    img_dest = os.path.join(output_dir, \"images\", split)\n",
    "    lbl_dest = os.path.join(output_dir, \"labels\", split)\n",
    "\n",
    "    for img_file in image_list:\n",
    "        img_path = os.path.join(images_dir, img_file)\n",
    "        lbl_path = os.path.join(labels_dir, os.path.splitext(img_file)[0] + \".txt\")  # Corresponding label file\n",
    "\n",
    "        # Move image\n",
    "        shutil.move(img_path, os.path.join(img_dest, img_file))\n",
    "\n",
    "        # Move label if exists\n",
    "        if os.path.exists(lbl_path):\n",
    "            shutil.move(lbl_path, os.path.join(lbl_dest, os.path.basename(lbl_path)))\n",
    "\n",
    "# Move images & labels to respective folders\n",
    "move_files(train_images, \"train\")\n",
    "move_files(val_images, \"val\")\n",
    "\n",
    "print(f\"‚úÖ Dataset split completed! üöÄ\")\n",
    "print(f\"üü¢ Training set: {len(train_images)} images\")\n",
    "print(f\"üîµ Validation set: {len(val_images)} images\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating dataset.yaml File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `dataset.yaml` file defines the structure of our dataset, specifying the paths to training and validation images along with the class names for object detection. This file is essential for configuring object detection models like YOLO.\n",
    "\n",
    "#### `dataset.yaml` File Structure:\n",
    "```yaml\n",
    "train: dataset/Processed_dataset/images/train\n",
    "val: dataset/Processed_dataset/images/val\n",
    "\n",
    "names:\n",
    "  0: A1\n",
    "  1: A10\n",
    "  2: A11\n",
    "  3: A12\n",
    "  4: A13\n",
    "  5: A14\n",
    "  6: A15\n",
    "  7: A16\n",
    "  8: A17\n",
    "  9: A18\n",
    "  10: A19\n",
    "  11: A2\n",
    "  12: A20\n",
    "  13: A3\n",
    "  14: A4\n",
    "  15: A5\n",
    "  16: A6\n",
    "  17: A7\n",
    "  18: A8\n",
    "  19: A9\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a YOLO11n-OBB Model on the Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This code initializes a YOLO11n-OBB model from scratch using a specified configuration file (yolo11n-obb.yaml). The model is then trained on the [Military Aircraft Recognition dataset](https://www.kaggle.com/datasets/khlaifiabilel/military-aircraft-recognition-dataset) for 50 epochs with an image size of 1024x1024 and a batch size of 4. The validation step is disabled (val=False) during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new YOLO11n-OBB model from scratch\n",
    "model = YOLO(\"yolo11n-obb.yaml\")\n",
    "\n",
    "# Train the model on the DOTAv1 dataset\n",
    "results = model.train(data=\"dataset.yaml\", epochs=50, imgsz=1024, batch=4, val=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validating the YOLO11n-OBB Model on the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the model\n",
    "metrics = model.val(data=\"dataset.yaml\")  # no arguments needed, dataset and settings remembered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YOLO11n-OBB Training Summary\n",
    "\n",
    "**Environment:**\n",
    "- **Ultralytics Version:** 8.3.70  \n",
    "- **Python Version:** 3.9.21  \n",
    "- **Torch Version:** 2.5.1+cu121  \n",
    "- **GPU:** NVIDIA GeForce RTX 3050 Laptop GPU (4096MiB)  \n",
    "\n",
    "## Model Summary\n",
    "- **YOLO11n-OBB (Fused):** 257 layers, 2,657,623 parameters, 0 gradients, 6.6 GFLOPs  \n",
    "\n",
    "---\n",
    "\n",
    "## Validation Results\n",
    "- **Total Images Scanned:** 768  \n",
    "- **Background Images:** 1  \n",
    "- **Corrupt Images:** 0  \n",
    "\n",
    "### Performance Metrics:\n",
    "\n",
    "| Class  | Images | Instances | Precision (P) | Recall (R) | mAP@50 | mAP@50-95 |\n",
    "|--------|--------|-----------|--------------|------------|--------|------------|\n",
    "| **All** | 769 | 4354 | **0.896** | **0.858** | **0.928** | **0.771** |\n",
    "| A1  | 64 | 277 | 0.82 | 0.856 | 0.926 | 0.756 |\n",
    "| A10 | 42 | 213 | 0.962 | 0.949 | 0.985 | 0.851 |\n",
    "| A11 | 48 | 127 | 0.793 | 0.772 | 0.87  | 0.785 |\n",
    "| A12 | 35 | 112 | 0.961 | 0.786 | 0.908 | 0.684 |\n",
    "| A13 | 62 | 385 | 0.867 | 0.914 | 0.947 | 0.716 |\n",
    "| A14 | 96 | 358 | 0.933 | 0.944 | 0.979 | 0.866 |\n",
    "| A15 | 27 | 108 | 0.717 | 0.656 | 0.73  | 0.536 |\n",
    "| A16 | 58 | 518 | 0.946 | 0.942 | 0.98  | 0.779 |\n",
    "| A17 | 66 | 253 | 0.927 | 0.988 | 0.978 | 0.864 |\n",
    "| A18 | 20 | 57  | 0.844 | 0.649 | 0.795 | 0.7   |\n",
    "| A19 | 66 | 211 | 0.842 | 0.659 | 0.852 | 0.661 |\n",
    "| A2  | 78 | 301 | 0.961 | 0.973 | 0.99  | 0.832 |\n",
    "| A20 | 44 | 198 | 0.921 | 0.821 | 0.907 | 0.597 |\n",
    "| A3  | 70 | 285 | 0.961 | 0.942 | 0.985 | 0.845 |\n",
    "| A4  | 31 | 108 | 0.85  | 0.895 | 0.937 | 0.825 |\n",
    "| A5  | 48 | 234 | 0.943 | 0.712 | 0.911 | 0.696 |\n",
    "| A6  | 18 | 71  | 0.876 | 0.93  | 0.967 | 0.866 |\n",
    "| A7  | 45 | 149 | 0.913 | 0.799 | 0.932 | 0.844 |\n",
    "| A8  | 37 | 181 | 0.949 | 0.994 | 0.994 | 0.853 |\n",
    "| A9  | 45 | 208 | 0.928 | 0.981 | 0.988 | 0.864 |\n",
    "\n",
    "---\n",
    "\n",
    "## Processing Speed\n",
    "- **Preprocessing:** 1.4ms per image  \n",
    "- **Inference:** 16.0ms per image  \n",
    "- **Loss Calculation:** 0.0ms per image  \n",
    "- **Postprocessing:** 3.5ms per image  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Inference and Visualizing YOLO Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This code loads a fine-tuned YOLO11n-OBB model and performs batch inference on all images in the \"test-images\" folder. The model is loaded from a custom-trained checkpoint (best.pt). All images in the folder are processed and resized to 400x400 pixels for uniform display. The results are visualized using Matplotlib in a single row of subplots. The predictions, including detected objects with bounding boxes, will be displayed inside the notebook for easy review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fine-tuned model\n",
    "model = YOLO(\"yolo11n-obb.pt\")  # load an official model\n",
    "model = YOLO(\"runs/obb/train/weights/best.pt\")  # load a custom model\n",
    "\n",
    "# Define folder containing images\n",
    "image_folder = \"test-images\"  # Update path\n",
    "# Get all image paths (supports jpg, png, jpeg)\n",
    "image_paths = [os.path.join(image_folder, f) for f in os.listdir(image_folder) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "# Run inference on all images\n",
    "results = model(image_paths)  # Batch prediction\n",
    "\n",
    "# Resize settings\n",
    "resize_width, resize_height = 400, 400  # Adjust as needed\n",
    "\n",
    "# Processed images list\n",
    "processed_images = []\n",
    "for r in results:\n",
    "    im_bgr = r.plot()  # Get prediction image (BGR format)\n",
    "    im_rgb = Image.fromarray(im_bgr[..., ::-1])  # Convert BGR to RGB\n",
    "    im_rgb = im_rgb.resize((resize_width, resize_height))  # Resize\n",
    "    processed_images.append(im_rgb)\n",
    "\n",
    "# üìå Display all images in a row inside Jupyter Notebook\n",
    "fig, axes = plt.subplots(1, len(processed_images), figsize=(len(processed_images) * 5, 5))\n",
    "\n",
    "# Ensure axes is iterable for a single image case\n",
    "if len(processed_images) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Show each image in a subplot\n",
    "for ax, img in zip(axes, processed_images):\n",
    "    ax.imshow(img)\n",
    "    ax.axis(\"off\")  # Hide axis\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()  # üöÄ This ensures images are displayed inside Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![YOLO Prediction](output.png)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ultralytics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
